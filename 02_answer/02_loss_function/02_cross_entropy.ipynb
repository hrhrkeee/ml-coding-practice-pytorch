{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Cross Entropy Loss\n",
    "\n",
    "クロスエントロピー損失 (Cross Entropy Loss) は、多クラス分類問題で使われる損失関数です。\n",
    "LogSoftmax + NLLLoss の組み合わせと等価です。\n",
    "\n",
    "## Forward\n",
    "\n",
    "$$\n",
    "\\text{Loss} = -\\frac{1}{N} \\sum_{i=1}^{N} \\log\\left(\\frac{e^{x_{i,y_i}}}{\\sum_{j} e^{x_{i,j}}}\\right)\n",
    "$$\n",
    "\n",
    "## Backward\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_{i,j}} = \\frac{1}{N} (\\text{softmax}(x_i)_j - \\mathbb{1}_{j=y_i})\n",
    "$$\n",
    "\n",
    "ただし、$\\mathbb{1}_{j=y_i}$ は、$j=y_i$ のとき 1、それ以外は 0 です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from notebook_setup import test\n",
    "\n",
    "@test(\"02_loss_function.test_02_cross_entropy\", filter_name=\"forward\")\n",
    "def cross_entropy_forward(logits: np.ndarray, targets: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    logits  : np.ndarray : shape (N, C) - ロジット (未正規化スコア)\n",
    "    targets : np.ndarray : shape (N,)   - クラスラベル (0 ~ C-1)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss : float\n",
    "    \"\"\"\n",
    "    loss = None\n",
    "\n",
    "    # ここにコードを記述\n",
    "    # ---------------------------------------- #\n",
    "    N = logits.shape[0]\n",
    "    # 数値安定性のため、最大値を引く\n",
    "    logits_shifted = logits - np.max(logits, axis=1, keepdims=True)\n",
    "    exp_logits = np.exp(logits_shifted)\n",
    "    sum_exp = np.sum(exp_logits, axis=1)\n",
    "    log_sum_exp = np.log(sum_exp)\n",
    "    \n",
    "    # 各サンプルについて正解クラスのロジットを取得\n",
    "    correct_logits = logits_shifted[np.arange(N), targets.astype(int)]\n",
    "    loss = -np.mean(correct_logits - log_sum_exp)\n",
    "    # ---------------------------------------- #\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from notebook_setup import test\n",
    "\n",
    "@test(\"02_loss_function.test_02_cross_entropy\", filter_name=\"backward\")\n",
    "def cross_entropy_backward(logits: np.ndarray, targets: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    logits  : np.ndarray : shape (N, C)\n",
    "    targets : np.ndarray : shape (N,)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    grad : np.ndarray : shape (N, C)\n",
    "    \"\"\"\n",
    "    grad = None\n",
    "\n",
    "    # ここにコードを記述\n",
    "    # ---------------------------------------- #\n",
    "    N = logits.shape[0]\n",
    "    # Softmax の計算\n",
    "    logits_shifted = logits - np.max(logits, axis=1, keepdims=True)\n",
    "    exp_logits = np.exp(logits_shifted)\n",
    "    softmax = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "    \n",
    "    # 勾配の計算\n",
    "    grad = softmax.copy()\n",
    "    grad[np.arange(N), targets.astype(int)] -= 1\n",
    "    grad /= N\n",
    "    # ---------------------------------------- #\n",
    "\n",
    "    return grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
